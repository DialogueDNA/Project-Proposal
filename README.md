
# AI-Driven Multi-Speaker Identification and Emotion Analysis System

## Project Overview
Our project aims to develop an AI-driven software system that identifies and distinguishes between different speakers at the start of a conversation, lesson, or meeting. The system analyzes the speakers’ voices and assigns a unique identity to each participant. Additionally, the system generates an individual report for each participant, which includes:

- **Emotion analysis**: Tracks the participant’s emotional state based on tone and content.
- **Key point summary**: Summarizes key contributions made by each participant.
- **Trigger analysis**: Identifies which parts of the conversation led to emotional changes.

---

## Problem Statement
In multi-person conversations, it’s often difficult to track who is speaking, what each person contributed, and how they emotionally responded to the discussion. This is crucial for meetings, lessons, and negotiations where participant engagement, emotional responses, and key takeaways are essential.

Existing tools like voice assistants, transcription services, and emotion analysis systems each address part of the problem. However, our solution integrates **speaker identification**, **speech-to-text transcription**, **emotion analysis**, and **personalized reporting** into a comprehensive system.

---

## Solution
Our AI-powered system prompts each participant to record a short sentence for voice identification. The system tracks speaker contributions, converts speech to text, and performs emotion recognition throughout the session. At the end of the session, each participant receives a personalized report summarizing their contributions, emotional trends, and moments that triggered emotional shifts.

### Key Features
- **Voice Registration**: Participants register their voice at the start of a session.
- **Speaker Tracking**: Identify and track individual speakers throughout the conversation.
- **Speech-to-Text**: Real-time transcription of all spoken words.
- **Emotion Detection**: Detect and track the emotional state of each participant.
- **Trigger Analysis**: Identify moments in the conversation that led to emotional changes.
- **Personalized Reports**: Each participant receives a personalized report including key contributions, emotional trends, and trigger analysis.

---

## Expected Users
- **Teachers & Educators**: Track student engagement and emotional responses during lessons.
- **Business Professionals**: Summarize key points in meetings and analyze emotional responses to negotiations or presentations.
- **Therapists & Counselors**: Track emotional shifts and participant involvement in group therapy sessions.
- **Market Researchers**: Analyze customer sentiment in focus groups or interviews.

---

## Technology Stack
- **Programming Languages**: Python (for AI/ML models), JavaScript (for front-end development), SQL (for database management).
- **APIs & Services**: Google Speech-to-Text, Microsoft’s Emotion API, IBM Watson (for NLP).
- **Cloud Storage**: AWS S3 or Google Cloud Storage for recording and report storage.
- **AI/ML Tools**: TensorFlow or PyTorch for machine learning models for speaker and emotion recognition.

---

## Implementation Approach
1. **Agile Development**: Iterative development using sprints to focus on core features.
2. **Data Collection & Training**: Collect diverse voice data to train speaker and emotion recognition models.
3. **System Integration**: Combine speech-to-text, speaker recognition, and emotion tracking into one comprehensive system.
4. **Report Generation**: Generate personalized PDF/HTML reports for each participant at the end of the session.

---

## Challenges
- **Voice Identification**: Ensuring accurate voice identification, especially in the presence of overlapping voices.
- **Emotion Analysis**: Ensuring accurate and context-aware emotional analysis.
- **Data Privacy**: Ensuring participant data privacy and compliance with data protection standards.

---

## Impact & Benefits
- **Insightful Reports**: Provide users with insights into participant engagement, emotional states, and key contributions.
- **Efficiency**: Automate note-taking, emotional analysis, and participant tracking.
- **Improved Decision-Making**: Identify emotional triggers to improve negotiations, discussions, and engagement.

---

## Future Enhancements
- **Language Support**: Add support for multiple languages.
- **Custom Reports**: Allow users to customize participant reports.
- **Business Tool Integration**: Integrate with tools like Microsoft Teams or Zoom.

---

## Demonstration
A live demo will feature multiple participants registering their voices, participating in a brief conversation, and receiving personalized reports at the end of the session.

---

## Team
- **Amal Amsis** 
- **Yarden Daniel** 
- **Yaniv Zamir** 


